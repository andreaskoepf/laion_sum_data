{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download selected text summarization datasets from Huggingface\n",
    "\n",
    "\n",
    "- wikihow/all: [repo](https://github.com/mahnazkoupaee/WikiHow-Dataset), paper: [WikiHow: A Large Scale Text Summarization Dataset](https://arxiv.org/abs/1810.09305), HF: [wikihow](https://huggingface.co/datasets/wikihow), requires manual download\n",
    "- xsum: HF: [xsum](https://huggingface.co/datasets/xsum)\n",
    "- cnn-dailymail: repo: [abisee/cnn-dailymail](https://github.com/abisee/cnn-dailymail), HF: [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail)\n",
    "- samsum: paper: [SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization](https://arxiv.org/abs/1911.12237), HF: [samsum](https://huggingface.co/datasets/samsum)\n",
    "- scitldr: repo: [allenai/scitldr](https://github.com/allenai/scitldr), paper: [TLDR: Extreme Summarization of Scientific Documents](https://arxiv.org/abs/2004.15011), HF: [scitldr](https://huggingface.co/datasets/scitldr)\n",
    "- billsum: HF: [billsum](https://huggingface.co/datasets/billsum)\n",
    "\n",
    "\n",
    "Huggingface datastes for summarization: https://huggingface.co/datasets?task_categories=task_categories:summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hf_dataset_key</th>\n",
       "      <th>source_key</th>\n",
       "      <th>target_key</th>\n",
       "      <th>flan_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikihow/all</td>\n",
       "      <td>text</td>\n",
       "      <td>headline</td>\n",
       "      <td>Produce an article summary including outlines ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xsum/1.2.1</td>\n",
       "      <td>document</td>\n",
       "      <td>summary</td>\n",
       "      <td>Given the following news article, summarize th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnn_dailymail/3.0.0</td>\n",
       "      <td>article</td>\n",
       "      <td>highlights</td>\n",
       "      <td>Produce an article summary of the following ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>samsum</td>\n",
       "      <td>dialogue</td>\n",
       "      <td>summary</td>\n",
       "      <td>Briefly summarize in third person the followin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scitldr/AIC</td>\n",
       "      <td>source</td>\n",
       "      <td>target</td>\n",
       "      <td>Given the following scientific article, provid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>billsum</td>\n",
       "      <td>text</td>\n",
       "      <td>summary</td>\n",
       "      <td>Summarize the following proposed legislation (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hf_dataset_key source_key  target_key  \\\n",
       "0          wikihow/all       text    headline   \n",
       "1           xsum/1.2.1   document     summary   \n",
       "2  cnn_dailymail/3.0.0    article  highlights   \n",
       "3               samsum   dialogue     summary   \n",
       "4          scitldr/AIC     source      target   \n",
       "5              billsum       text     summary   \n",
       "\n",
       "                                         flan_prompt  \n",
       "0  Produce an article summary including outlines ...  \n",
       "1  Given the following news article, summarize th...  \n",
       "2  Produce an article summary of the following ne...  \n",
       "3  Briefly summarize in third person the followin...  \n",
       "4  Given the following scientific article, provid...  \n",
       "5  Summarize the following proposed legislation (...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load hf datasets csv file\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# from https://github.com/jordiclive/Summarization/blob/704c079892faa7f902710d8b68e781b856adfa5c/processing/hf_datasets.csv\n",
    "datasets_info = pd.read_csv(\"hf_datasets/hf_datasets.csv\")\n",
    "datasets_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading wikihow/all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration all-3003e4082f016f00\n",
      "Found cached dataset wikihow (/media/koepf/data2/laion/hf_data_cache/wikihow/all-3003e4082f016f00/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f04f9746a8c4331bbfec41d020f3115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['text', 'headline', 'title']\n",
      "dataset: wikihow/all; split: train; rows: 157.3k;\n",
      "writing: data/wikihow/wikihow-all_train.snappy.parquet\n",
      "dataset: wikihow/all; split: validation; rows: 5.6k;\n",
      "writing: data/wikihow/wikihow-all_validation.snappy.parquet\n",
      "dataset: wikihow/all; split: test; rows: 5.6k;\n",
      "writing: data/wikihow/wikihow-all_test.snappy.parquet\n",
      "loading xsum/1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration 1.2.1\n",
      "Found cached dataset xsum (/media/koepf/data2/laion/hf_data_cache/xsum/1.2.1/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f2373cc81e4e2d983989ca68bc9eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['document', 'summary', 'id']\n",
      "dataset: xsum/1.2.1; split: train; rows: 204.0k;\n",
      "writing: data/xsum/xsum-1.2.1_train.snappy.parquet\n",
      "dataset: xsum/1.2.1; split: validation; rows: 11.3k;\n",
      "writing: data/xsum/xsum-1.2.1_validation.snappy.parquet\n",
      "dataset: xsum/1.2.1; split: test; rows: 11.3k;\n",
      "writing: data/xsum/xsum-1.2.1_test.snappy.parquet\n",
      "loading cnn_dailymail/3.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/media/koepf/data2/laion/hf_data_cache/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fae60a644204c6486fb9599bb467ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['article', 'highlights', 'id']\n",
      "dataset: cnn_dailymail/3.0.0; split: train; rows: 287.1k;\n",
      "writing: data/cnn_dailymail/cnn_dailymail-3.0.0_train.snappy.parquet\n",
      "dataset: cnn_dailymail/3.0.0; split: validation; rows: 13.4k;\n",
      "writing: data/cnn_dailymail/cnn_dailymail-3.0.0_validation.snappy.parquet\n",
      "dataset: cnn_dailymail/3.0.0; split: test; rows: 11.5k;\n",
      "writing: data/cnn_dailymail/cnn_dailymail-3.0.0_test.snappy.parquet\n",
      "loading samsum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/media/koepf/data2/laion/hf_data_cache/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cea2542162e4955a3894895a45f4ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['id', 'dialogue', 'summary']\n",
      "dataset: samsum; split: train; rows: 14.7k;\n",
      "writing: data/samsum/samsum_train.snappy.parquet\n",
      "dataset: samsum; split: validation; rows: 0.8k;\n",
      "writing: data/samsum/samsum_validation.snappy.parquet\n",
      "dataset: samsum; split: test; rows: 0.8k;\n",
      "writing: data/samsum/samsum_test.snappy.parquet\n",
      "loading scitldr/AIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scitldr (/media/koepf/data2/laion/hf_data_cache/scitldr/AIC/0.0.0/79e0fa75961392034484808cfcc8f37deb15ceda153b798c92d9f621d1042fef)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ef7794534b48d38ec651a46177fa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['source', 'source_labels', 'rouge_scores', 'paper_id', 'ic', 'target']\n",
      "dataset: scitldr/AIC; split: train; rows: 2.0k;\n",
      "writing: data/scitldr/scitldr-AIC_train.snappy.parquet\n",
      "dataset: scitldr/AIC; split: validation; rows: 0.6k;\n",
      "writing: data/scitldr/scitldr-AIC_validation.snappy.parquet\n",
      "dataset: scitldr/AIC; split: test; rows: 0.6k;\n",
      "writing: data/scitldr/scitldr-AIC_test.snappy.parquet\n",
      "loading billsum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset billsum (/media/koepf/data2/laion/hf_data_cache/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b605daa093a48f6894fae8d299d0f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation set missing for billsum, sampling synthetic validation (size: 631).\n",
      "columns: ['text', 'summary', 'title']\n",
      "dataset: billsum; split: train; rows: 18.3k;\n",
      "writing: data/billsum/billsum_train.snappy.parquet\n",
      "dataset: billsum; split: validation; rows: 0.6k;\n",
      "writing: data/billsum/billsum_validation.snappy.parquet\n",
      "dataset: billsum; split: test; rows: 3.3k;\n",
      "writing: data/billsum/billsum_test.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import json\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "cache_dir = str(Path('../hf_data_cache/').resolve())\n",
    "\n",
    "additional_args = {\n",
    "    'wikihow/all': {\n",
    "        'data_dir': str(Path('../WikiHow-Dataset/').resolve())\n",
    "    }\n",
    "}\n",
    "\n",
    "provenance_id_colums = {\n",
    "    'wikihow/all': ['title'],\n",
    "    'xsum/1.2.1': ['id'],\n",
    "    'cnn_dailymail/3.0.0': ['id'],\n",
    "    'samsum': ['id'],\n",
    "    'scitldr/AIC': ['paper_id'],\n",
    "    'billsum': ['title'],\n",
    "}\n",
    "\n",
    "\n",
    "def convert_dataset(data, text_key, summary_key, output_dir: Path, output_prefix: str, id_colums: List[str], compression:str=\"snappy\"):\n",
    "    fn = f'{output_prefix}.{compression}.parquet'\n",
    "    fn = output_dir / fn\n",
    "    \n",
    "    text, summary, provenance = [], [], []\n",
    "    provenance = []\n",
    "    for idx, row in data.iterrows():\n",
    "        t = row[text_key]\n",
    "        if isinstance(t, np.ndarray):    # special list handling for scitldr\n",
    "            if t.size == 1:\n",
    "                t = t.item()\n",
    "            else:\n",
    "                t = ' '.join(t)\n",
    "            assert type(t) == str\n",
    "        text.append(t)\n",
    "\n",
    "        s = row[summary_key]\n",
    "        if isinstance(s, np.ndarray):   # special array handling for scitldr\n",
    "            if s.size == 1:\n",
    "                s = s.item()\n",
    "            else:\n",
    "                s = random.choice(s)\n",
    "\n",
    "            assert type(s) == str\n",
    "        summary.append(s)\n",
    "        p = { 'src': output_prefix }\n",
    "        for col in id_colums:\n",
    "            p[col] = row[col]\n",
    "        provenance.append(json.dumps(p))\n",
    "    \n",
    "    text_, summary_, provenance_ = map(lambda x: pd.array(x, dtype=\"string\"), (text, summary, provenance))\n",
    "    df = pd.DataFrame({\"text\": text_, \"summary\": summary_, \"provenance\": provenance_})\n",
    "    print(f'writing: {fn}')\n",
    "    #print(df.head())\n",
    "    df.to_parquet(\n",
    "        fn, \n",
    "        engine=\"pyarrow\",\n",
    "        compression=compression\n",
    "    )\n",
    "\n",
    "\n",
    "# load datasets\n",
    "for index, row in datasets_info.iterrows():\n",
    "    dataset_name = row['hf_dataset_key']\n",
    "    #if dataset_name != 'scitldr/AIC':       # for single dataset debugging\n",
    "    #    continue\n",
    "\n",
    "    extra_args = {}\n",
    "    if dataset_name in additional_args:\n",
    "        extra_args = additional_args[dataset_name]\n",
    "\n",
    "    print(f'loading {dataset_name}')\n",
    "    name = dataset_name.split(\"/\")\n",
    "    if len(name) > 1:\n",
    "        data = datasets.load_dataset(name[0], name=name[1], cache_dir=cache_dir, **extra_args)\n",
    "    else:\n",
    "        data = datasets.load_dataset(name[0], cache_dir=cache_dir, **extra_args)\n",
    "\n",
    "    # make sure every dataset has a validation set, sample one if missing\n",
    "    splits = {}\n",
    "    split_names = ['train', 'validation', 'test']\n",
    "    min_num_val = 100   # min size of valiadion set\n",
    "    \n",
    "    if 'validation' not in data.keys():        \n",
    "        train = data['train'].to_pandas()\n",
    "        val_size = max(int(len(train)//30), min_num_val)\n",
    "        print(f'Warning: Validation set missing for {dataset_name}, sampling synthetic validation (size: {val_size}).')\n",
    "        train, val = train_test_split(train, test_size=val_size)\n",
    "        splits['train'] = train\n",
    "        splits['validation'] = val\n",
    "    else:\n",
    "        splits['train'] = data['train'].to_pandas()\n",
    "        splits['validation'] = data['validation'].to_pandas()\n",
    "    splits['test'] = data['test'].to_pandas()\n",
    "\n",
    "    print('columns:', data['train'].column_names)\n",
    "\n",
    "    for split_name, split_df in splits.items():\n",
    "        print(f'dataset: {dataset_name}; split: {split_name}; rows: {len(split_df)/1000:.1f}k;')\n",
    "\n",
    "        prefix = dataset_name.replace('/', '-')\n",
    "        prefix = prefix + '_' + split_name\n",
    "\n",
    "        output_dir = Path(f'./data/{name[0]}')\n",
    "        if not output_dir.exists():\n",
    "            print(f'creating directory: {output_dir}')\n",
    "            output_dir.mkdir(exist_ok=True)\n",
    "        id_colums = provenance_id_colums[dataset_name]\n",
    "        convert_dataset(split_df, row['source_key'], row['target_key'], output_dir, prefix, id_colums) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('news_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52837886e5e45ddaef7e1045db27270fbeb0fd633b908ac870aa34fbd32295eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
