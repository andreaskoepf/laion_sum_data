{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cornell Newsroom Summarization Dataset\n",
    "\n",
    "- Website: https://lil.nlp.cornell.edu/newsroom/index.html\n",
    "- Paper: https://aclanthology.org/N18-1065/\n",
    "- Repo: https://github.com/lil-lab/newsroom (how to read the dataset is explained in the readme)\n",
    "\n",
    "\n",
    "\n",
    "| Name | Value |\n",
    "| --- | --- |\n",
    "| Dataset Size | 1,321,995 articles |\n",
    "| Training Set Size | 995,041 articles |\n",
    "| Mean Article Length | 658.6 words |\n",
    "| Mean Summary Length | 26.7 words |\n",
    "| Total Vocabulary Size | 6,925,712 words |\n",
    "| Occurring 10+ Times | 784,884 words |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from newsroom import jsonl\n",
    "\n",
    "\n",
    "def convert_newsroom_dataset(input_path, output_dir, output_prefix: str, max_rows_per_file:int=400000, compression:str=\"snappy\"):\n",
    "    part = 0\n",
    "    text, summary, provenance = [], [], []\n",
    "    empty_summaries = 0\n",
    "\n",
    "    def write_part(with_part_postfix: bool):\n",
    "        if with_part_postfix:\n",
    "            fn = f'{output_prefix}_{part:05d}.{compression}.parquet'\n",
    "        else:\n",
    "            fn = f'{output_prefix}.{compression}.parquet'\n",
    "        \n",
    "        fn = output_dir / fn\n",
    "        text_, summary_, provenance_ = map(lambda x: pd.array(x, dtype=\"string\"), (text, summary, provenance))\n",
    "        df = pd.DataFrame({\"text\": text_, \"summary\": summary_, \"provenance\": provenance_})\n",
    "        print(f'writing: {fn} (rows: {len(df)}; empty summaries: {empty_summaries} (skipped))')\n",
    "        df.to_parquet(\n",
    "            fn, \n",
    "            engine=\"pyarrow\",\n",
    "            compression=compression\n",
    "        )\n",
    "\n",
    "    with jsonl.open(str(input_path), gzip = True) as f:\n",
    "        for entry in f:\n",
    "            \n",
    "            s = entry['summary']\n",
    "            if s is None or len(s.strip()) == 0:\n",
    "                empty_summaries += 1\n",
    "                continue\n",
    "\n",
    "            text.append(entry['text'])\n",
    "            summary.append(s)\n",
    "            provenance.append(json.dumps({ 'src': output_prefix, 'url': entry['archive']}))\n",
    "            if max_rows_per_file is not None and max_rows_per_file > 0 and len(text) > max_rows_per_file:\n",
    "                write_part(True)\n",
    "                part += 1\n",
    "                text, summary, provenance = [], [], []\n",
    "\n",
    "    if len(text) > 0:\n",
    "        write_part(part > 0)\n",
    "\n",
    "dataset_dir = Path('../newsroom-thin')\n",
    "output_dir = Path('./data/newsroom/')\n",
    "dataset_file_names = list(dataset_dir.glob('*.dataset'))\n",
    "output_prefixes = map(lambda fn: 'newsroom_' + fn.stem, dataset_file_names)\n",
    "\n",
    "#dataset_file_names = ['dev.dataset', 'test.dataset', 'train.dataset']\n",
    "#output_prefixes = ['newsroom_dev', 'newsroom_test', 'newsroom_train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing: data/newsroom/newsroom_dev.snappy.parquet (rows: 108599; empty summaries: 185 (skipped))\n",
      "writing: data/newsroom/newsroom_test.snappy.parquet (rows: 108670; empty summaries: 152 (skipped))\n"
     ]
    }
   ],
   "source": [
    "# run conversion:\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "for fn, prefix in zip(dataset_file_names, output_prefixes):\n",
    "    convert_newsroom_dataset(fn, output_dir, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: \"newsroom_dev.snappy.parquet\"; rows: 108599; mean_wordcount: { text: 612.10; summary: 26.93; }; mean_strlen: { text: 3764.11; summary: 163.75 };\n",
      "file: \"newsroom_test.snappy.parquet\"; rows: 108670; mean_wordcount: { text: 615.35; summary: 26.85; }; mean_strlen: { text: 3784.15; summary: 163.29 };\n"
     ]
    }
   ],
   "source": [
    "# check files in output dir\n",
    "\n",
    "def mean_strlen(col: pd.Series):\n",
    "    return col.apply(len).mean()\n",
    "\n",
    "def count_words(s):\n",
    "    return sum(1 for w in s.split(' ') if len(w) > 0)\n",
    "\n",
    "def mean_wordcount(col: pd.Series):\n",
    "    return col.apply(count_words).mean()\n",
    "\n",
    "\n",
    "for fn in output_dir.glob('*.parquet'):\n",
    "    df = pd.read_parquet(path=str(fn), engine='pyarrow')\n",
    "    text, summary = df[\"text\"], df[\"summary\"]\n",
    "    print(f'file: \"{fn.name}\"; rows: {len(df)}; mean_wordcount: {{ text: {mean_wordcount(text):.2f}; summary: {mean_wordcount(summary):.2f}; }}; mean_strlen: {{ text: {mean_strlen(text):.2f}; summary: {mean_strlen(summary):.2f} }};')\n",
    "\n",
    "#     df = pd.read_parquet(path=str(fn), engine='pyarrow')\n",
    "#     print('rows:', len(df))\n",
    "#     print(df.head(2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('news_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52837886e5e45ddaef7e1045db27270fbeb0fd633b908ac870aa34fbd32295eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
