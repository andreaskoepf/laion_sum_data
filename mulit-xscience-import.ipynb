{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi X-Science Dataset\n",
    "\n",
    "- Paper: https://arxiv.org/abs/2010.14235\n",
    "- Repo: https://github.com/yaolu/Multi-XScience\n",
    "\n",
    "Source-Format: json files\n",
    "\n",
    "Uncompressed sizes:\n",
    "\n",
    "```\n",
    " 28M test.json\n",
    "166M train.json\n",
    " 28M val.json\n",
    "\n",
    "Total: 221M\n",
    "```\n",
    "\n",
    "\n",
    "| Property | Value |\n",
    "| --- | --- |\n",
    "| # train | 30,369 |\n",
    "| # val | 5,066 |\n",
    "| # test | 5,093 |\n",
    "| doc. len (words) | 778.08 |\n",
    "| summ. len (words) |  116.44 |\n",
    "| # refs | 4.42 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# potentially different options for concatenating abstract + abstracts of referenced papers\n",
    "\n",
    "section_headers_empty = {\n",
    "    'query_abs': '',\n",
    "    'reference_abs': ''\n",
    "}\n",
    "\n",
    "section_headers_newline = {\n",
    "    'query_abs': '',\n",
    "    'reference_abs': '\\n'\n",
    "}\n",
    "\n",
    "section_headers_minimal = {\n",
    "    'query_abs': '',\n",
    "    'reference_abs': '\\n@cite: '\n",
    "}\n",
    "\n",
    "section_headers_marker = {\n",
    "    'query_abs': '### Abstract of query paper ###\\n',\n",
    "    'reference_abs': '\\n### Abstract of @cite ###\\n'\n",
    "}\n",
    "\n",
    "def convert_dataset(fn: Path, output_dir: Path, output_prefix: str, section_headers: Dict[str, str], anonymize_refs: bool=False, compression:str=\"snappy\"):\n",
    "    text, summary, provenance = [], [], []\n",
    "\n",
    "    missing_ref_abs = 0\n",
    "\n",
    "    with fn.open('r') as f:\n",
    "        entries = json.load(f)\n",
    "        for entry in entries:\n",
    "            abstract = entry['abstract']\n",
    "            related_work = entry['related_work']\n",
    "            refs = entry['ref_abstract']\n",
    "            \n",
    "            sb = [section_headers['query_abs'], abstract]\n",
    "            for rid, ref_data in refs.items():\n",
    "                ref_abstract = ref_data['abstract']\n",
    "                if len(ref_abstract.strip()) == 0:\n",
    "                    missing_ref_abs += 1\n",
    "\n",
    "                ref_header = section_headers['reference_abs']\n",
    "                if not anonymize_refs:\n",
    "                    ref_header = ref_header.replace('@cite', rid)\n",
    "                sb.append(ref_header)\n",
    "                sb.append(ref_abstract)\n",
    "\n",
    "            if anonymize_refs:\n",
    "                related_work = re.sub(r'@cite_[0-9]+', r'@cite', related_work)\n",
    "\n",
    "            text.append(''.join(sb))\n",
    "            summary.append(related_work)\n",
    "\n",
    "            # aid: arxiv id (e.g. 2010.14235)\n",
    "            # mid: microsoft academic graph id\n",
    "            provenance.append(json.dumps({ 'src': prefix, 'aid': entry['aid'], 'mid': entry['mid'] }))\n",
    "\n",
    "    fn = f'{output_prefix}.{compression}.parquet'\n",
    "    fn = output_dir / fn\n",
    "    text_, summary_, provenance_ = map(lambda x: pd.array(x, dtype=\"string\"), (text, summary, provenance))\n",
    "    df = pd.DataFrame({\"text\": text_, \"summary\": summary_, \"provenance\": provenance_})\n",
    "    print(f'writing: {fn} (entries: {len(entries)}; missing ref abstracts: {missing_ref_abs})')\n",
    "    df.to_parquet(\n",
    "        fn, \n",
    "        engine=\"pyarrow\",\n",
    "        compression=compression\n",
    "    )\n",
    "\n",
    "dataset_dir = Path('../Multi-XScience/data')\n",
    "output_dir = Path('./data/multixscience/')\n",
    "dataset_file_names = list(dataset_dir.glob('*.json'))\n",
    "output_prefixes = map(lambda fn: 'multixscience_' + fn.stem, dataset_file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing: data/multixscience/multixscience_train.snappy.parquet (entries: 30369; missing ref abstracts: 20023)\n",
      "writing: data/multixscience/multixscience_val.snappy.parquet (entries: 5066; missing ref abstracts: 3383)\n",
      "writing: data/multixscience/multixscience_test.snappy.parquet (entries: 5093; missing ref abstracts: 3403)\n"
     ]
    }
   ],
   "source": [
    "# run conversion:\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "for fn, prefix in zip(dataset_file_names, output_prefixes):\n",
    "    convert_dataset(fn, output_dir, prefix, section_headers_minimal, anonymize_refs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: \"multixscience_train.snappy.parquet\"; rows: 30369; mean_wordcount: { text: 700.62; summary: 105.89; }; mean_strlen: { text: 4754.78; summary: 699.99 };\n",
      "file: \"multixscience_val.snappy.parquet\"; rows: 5066; mean_wordcount: { text: 700.02; summary: 104.43; }; mean_strlen: { text: 4747.70; summary: 690.17 };\n",
      "file: \"multixscience_test.snappy.parquet\"; rows: 5093; mean_wordcount: { text: 690.02; summary: 105.77; }; mean_strlen: { text: 4671.70; summary: 697.67 };\n"
     ]
    }
   ],
   "source": [
    "# check files in output dir\n",
    "\n",
    "def mean_strlen(col: pd.Series):\n",
    "    return col.apply(len).mean()\n",
    "\n",
    "def count_words(s):\n",
    "    return sum(1 for w in s.split(' ') if len(w) > 0)\n",
    "\n",
    "def mean_wordcount(col: pd.Series):\n",
    "    return col.apply(count_words).mean()\n",
    "\n",
    "show_random_entry = False\n",
    "for fn in output_dir.glob('*.parquet'):\n",
    "    df = pd.read_parquet(path=str(fn), engine='pyarrow')\n",
    "    text, summary = df[\"text\"], df[\"summary\"]\n",
    "    print(f'file: \"{fn.name}\"; rows: {len(df)}; mean_wordcount: {{ text: {mean_wordcount(text):.2f}; summary: {mean_wordcount(summary):.2f}; }}; mean_strlen: {{ text: {mean_strlen(text):.2f}; summary: {mean_strlen(summary):.2f} }};')\n",
    "\n",
    "    if show_random_entry:\n",
    "        i = random.randint(0, len(df)-1)\n",
    "        print(f'Random entry #{i}:')\n",
    "        print('### text:', df.loc[i]['text'])\n",
    "        print('### summary:', df.loc[i]['summary'])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example raw source dataset entry:\n",
    "\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"aid\": \"cs9809108\",\n",
    "    \"mid\": \"2949225035\",\n",
    "    \"abstract\": \"We present our approach to the problem of how an agent, within an economic Multi-Agent System, can determine when it should behave strategically (i.e. learn and use models of other agents), and when it should act as a simple price-taker. We provide a framework for the incremental implementation of modeling capabilities in agents, and a description of the forms of knowledge required. The agents were implemented and different populations simulated in order to learn more about their behavior and the merits of using and learning agent models. Our results show, among other lessons, how savvy buyers can avoid being cheated'' by sellers, how price volatility can be used to quantitatively predict the benefits of deeper models, and how specific types of agent populations influence system behavior.\",\n",
    "    \"related_work\": \"Within the MAS community, some work @cite_15 has focused on how artificial AI-based learning agents would fare in communities of similar agents. For example, @cite_6 and @cite_8 show how agents can learn the capabilities of others via repeated interactions, but these agents do not learn to predict what actions other might take. Most of the work in MAS also fails to recognize the possible gains from using explicit agent models to predict agent actions. @cite_9 is an exception and gives another approach for using nested agent models. However, they do not go so far as to try to quantify the advantages of their nested models or show how these could be learned via observations. We believe that our research will bring to the foreground some of the common observations seen in these research areas and help to clarify the implications and utility of learning and using nested agent models.\",\n",
    "    \"ref_abstract\": {\n",
    "      \"@cite_9\": {\n",
    "        \"mid\": \"1528079221\",\n",
    "        \"abstract\": \"In multi-agent environments, an intelligent agent often needs to interact with other individuals or groups of agents to achieve its goals. Agent tracking is one key capability required for intelligent interaction. It involves monitoring the observable actions of other agents and inferring their unobserved actions, plans, goals and behaviors. This article examines the implications of such an agent tracking capability for agent architectures. It specifically focuses on real-time and dynamic environments, where an intelligent agent is faced with the challenge of tracking the highly flexible mix of goal-driven and reactive behaviors of other agents, in real-time. The key implication is that an agent architecture needs to provide direct support for flexible and efficient reasoning about other agents' models. In this article, such support takes the form of an architectural capability to execute the other agent's models, enabling mental simulation of their behaviors. Other architectural requirements that follow include the capabilities for (pseudo-) simultaneous execution of multiple agent models, dynamic sharing and unsharing of multiple agent models and high bandwidth inter-model communication. We have implemented an agent architecture, an experimental variant of the Soar integrated architecture, that conforms to all of these requirements. Agents based on this architecture have been implemented to execute two different tasks in a real-time, dynamic, multi-agent domain. The article presents experimental results illustrating the agents' dynamic behavior.\"\n",
    "      },\n",
    "      \"@cite_15\": {\n",
    "        \"mid\": \"2156109180\",\n",
    "        \"abstract\": \"I. Introduction, 488. \\u2014 II. The model with automobiles as an example, 489. \\u2014 III. Examples and applications, 492. \\u2014 IV. Counteracting institutions, 499. \\u2014 V. Conclusion, 500.\"\n",
    "      },\n",
    "      \"@cite_6\": {\n",
    "        \"mid\": \"1591263692\",\n",
    "        \"abstract\": \"The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. This paper outlines a gradual evolution in our formal conception of intelligence that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice.\"\n",
    "      },\n",
    "      \"@cite_8\": {\n",
    "        \"mid\": \"\",\n",
    "        \"abstract\": \"\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('news_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52837886e5e45ddaef7e1045db27270fbeb0fd633b908ac870aa34fbd32295eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
