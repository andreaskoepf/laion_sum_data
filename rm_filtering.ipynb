{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from filtering import filter\n",
    "from filtering import ContrieverScoring, RewardModelScoring\n",
    "\n",
    "\n",
    "device = torch.device('cuda', 0)\n",
    "hf_cache_dir = '../hf_model_cache'\n",
    "\n",
    "cs = ContrieverScoring(device=device, hf_cache_dir=hf_cache_dir)\n",
    "rs = RewardModelScoring(chkpt_dir='./reward_model/checkpoints/rnd_b48_last_do25_checkpoint_01_0006772/', device=device, hf_cache_dir=hf_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset files and add token length, contriever cos, dot and reward_model scores\n",
    "# list all dataset files in source_directory\n",
    "\n",
    "source_data_path = Path('./data')\n",
    "data_files = list(source_data_path.glob('*/*.parquet'))\n",
    "\n",
    "def load_file(fn):\n",
    "    df = pd.read_parquet(path=str(fn), engine='pyarrow', columns=['text', 'summary', 'provenance'])\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_file(data_files[0])\n",
    "texts = df.text.iloc[0:10].to_list()\n",
    "summaries = df.summary.iloc[0:10].to_list()\n",
    "\n",
    "cos_scores, dot_scores = cs.score_multiple(texts, summaries)\n",
    "reward_scores = rs.score_multiple(texts, summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (787 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[787, 579, 971, 267, 488, 670, 1094, 529, 444, 159]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_count(text_list):\n",
    "    text_tokens = rs.tokenizer(text_list, padding=False, truncation=False)\n",
    "    return [len(l) for l in text_tokens.input_ids]\n",
    "\n",
    "texts = df.text.iloc[0:10].to_list()\n",
    "summaries = df.summary.iloc[0:10].to_list()\n",
    "\n",
    "token_count(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/koepf/data2/laion/news_venv/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fragments import Fragments\n",
    "\n",
    "f = Fragments(summary='test', text='test 123')\n",
    "f.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: data/cnn_dailymail/cnn_dailymail-3.0.0_test.snappy.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/koepf/data2/laion/news_venv/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# new rows for augmented data frame\n",
    "@torch.no_grad()\n",
    "def augmente_data_frame(df):\n",
    "    num_text_tokens = []\n",
    "    num_summary_tokens = []\n",
    "    contriever_score_cos = []\n",
    "    contriever_score_dot = []\n",
    "    reward = []\n",
    "    density = []\n",
    "    compression = []\n",
    "    coverage = []\n",
    "\n",
    "    num_rows = len(df)\n",
    "    batch_size = 32\n",
    "    for i in range(0, num_rows, batch_size):\n",
    "        page = df.iloc[i:i+batch_size]\n",
    "        texts = page.text.to_list()\n",
    "        summaries = page.summary.to_list()\n",
    "        \n",
    "        num_text_tokens.extend(token_count(texts))\n",
    "        num_summary_tokens.extend(token_count(summaries))\n",
    "\n",
    "        c, d = cs.score_multiple(texts, summaries)\n",
    "        contriever_score_cos.extend(c.tolist())\n",
    "        contriever_score_dot.extend(d.tolist())\n",
    "\n",
    "        r = rs.score_multiple(texts, summaries)\n",
    "        reward.extend(r.tolist())\n",
    "\n",
    "        for t,s in zip(texts, summaries):\n",
    "            f = Fragments(summary=s, text=t)\n",
    "            density.append(f.density())\n",
    "            coverage.append(f.coverage())\n",
    "            compression.append(f.compression())\n",
    "\n",
    "    assert all(len(x) == num_rows for x in (num_text_tokens, num_summary_tokens, contriever_score_cos, contriever_score_dot, reward, density, compression, coverage))\n",
    "\n",
    "    df['t5_text_token_count'] = pd.array(num_text_tokens, dtype=\"int\")\n",
    "    df['t5_summary_token_count'] = pd.array(num_summary_tokens, dtype=\"int\")\n",
    "    df['contriever_cos'] = pd.array(contriever_score_cos, dtype=\"float\")\n",
    "    df['contriever_dot'] = pd.array(contriever_score_dot, dtype=\"float\")\n",
    "    df['reward'] = pd.array(reward, dtype=\"float\")\n",
    "    df['density'] = pd.array(density, dtype=\"float\")\n",
    "    df['compression'] = pd.array(compression, dtype=\"float\")\n",
    "    df['coverage'] = pd.array(coverage, dtype=\"float\")\n",
    "    return df\n",
    "\n",
    "def augment_data_file(fn: Path, compression = 'snappy'):\n",
    "    fn = Path(fn)\n",
    "\n",
    "    print(f'reading: {fn}')\n",
    "    df = load_file(fn)\n",
    "\n",
    "    df = augmente_data_frame(df)\n",
    "    \n",
    "    old_postfix = '.snappy.parquet'\n",
    "    new_postfix = f'_scored.{compression}.parquet'\n",
    "    out_fn = fn.parent / fn.name.replace(old_postfix, new_postfix)\n",
    "\n",
    "    print(f'writing part: {out_fn} ({len(df)} rows)')\n",
    "    df.to_parquet(out_fn, compression=compression, engine=\"pyarrow\", row_group_size=1000)\n",
    "\n",
    "\n",
    "for fn in data_files:\n",
    "    augment_data_file(fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('news_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52837886e5e45ddaef7e1045db27270fbeb0fd633b908ac870aa34fbd32295eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
